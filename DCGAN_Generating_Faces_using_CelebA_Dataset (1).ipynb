{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fApsN9gm-bkJ"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0XuIgr0izgj"
      },
      "outputs": [],
      "source": [
        "#### optional\n",
        "!pip install wandb -qqq\n",
        "import wandb\n",
        "wandb.login(key='b02663b1171d397af9ccb1b6c8ab12c845835e77')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xT5WrC1n-g4A"
      },
      "outputs": [],
      "source": [
        "# importing the libraries\n",
        "import torch, torchvision, os, PIL, pdb\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBTTx5SK-UMq"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSsUyS2z-TtR"
      },
      "outputs": [],
      "source": [
        "### hyperparameters and general parameters\n",
        "n_epochs=10000\n",
        "batch_size=64\n",
        "lr=1e-5\n",
        "z_dim=200 #--- dimension of noise vector\n",
        "device='cuda' #GPU\n",
        "\n",
        "cur_step=0\n",
        "crit_cycles=5 #--- we train critic 5 times + 1 train of generator - so that critic not overpowered by generator\n",
        "gen_losses=[] #--- to append loss value of generator\n",
        "crit_losses=[]#--- to append loss value of critic\n",
        "show_step=1\n",
        "save_step=1\n",
        "\n",
        "wandbact=1 # yes, we want to track stats through weights and biases, optional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx3zW0S5_FTI"
      },
      "source": [
        "## Setup Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_QS6hRNwXFd"
      },
      "outputs": [],
      "source": [
        "#--- Visualization Function\n",
        "def show(tensor, num=25, wandbactive=0, name=''):\n",
        "  data = tensor.detach().cpu() \n",
        "  grid = make_grid(data[:num], nrow=5).permute(1,2,0)\n",
        "\n",
        "  ## optional\n",
        "  if (wandbactive==1):\n",
        "    wandb.log({name:wandb.Image(grid.numpy().clip(0,1))})\n",
        "\n",
        "  plt.imshow(grid.clip(0,1))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJX0alVUl8VQ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "experiment_name = wandb.util.generate_id()\n",
        "\n",
        "myrun=wandb.init(\n",
        "    project=\"wgan_10000_64\",\n",
        "    group=experiment_name,\n",
        "    config={\n",
        "        \"optimizer\":\"adam\",\n",
        "        \"model\":\"wgan gp\",\n",
        "        \"epoch\":\"10000\",\n",
        "        \"batch_size\":64\n",
        "    }\n",
        ")\n",
        "\n",
        "config=wandb.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sayk6mxd_QIY"
      },
      "outputs": [],
      "source": [
        "print(experiment_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLlBWV2O_aRf"
      },
      "source": [
        "# **Generator Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jb_krOBKm2Wv"
      },
      "outputs": [],
      "source": [
        "# generator model\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, z_dim=200, d_dim=16):\n",
        "    super(Generator, self).__init__()\n",
        "    self.z_dim=z_dim\n",
        "\n",
        "    self.gen = nn.Sequential(\n",
        "            ## ConvTranspose2d: in_channels, out_channels, kernel_size, stride=1, padding=0\n",
        "            ## Calculating new width and height: (n-1)*stride -2*padding +ks\n",
        "            ## n = width or height\n",
        "            ## ks = kernel size\n",
        "            ## we begin with a 1x1 image with z_dim number of channels (200) - initlalized z_dim = 200 | 1x1x200\n",
        "            ##  - we decrease no. of channels but increase size of image\n",
        "\n",
        "            nn.ConvTranspose2d(z_dim, d_dim * 32, 4, 1, 0), ## 4x4 image (ch: 200 to 512) | 4x4x512\n",
        "            nn.BatchNorm2d(d_dim*32),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(d_dim*32, d_dim*16, 4, 2, 1), ## 8x8 image (ch: 512 to 256) | 8x8x256\n",
        "            nn.BatchNorm2d(d_dim*16),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(d_dim*16, d_dim*8, 4, 2, 1), ## 16x16 image (ch: 256 to 128) | 16x16x128\n",
        "            #(n-1)*stride -2*padding +ks = (8-1)*2-2*1+4=16\n",
        "            nn.BatchNorm2d(d_dim*8),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(d_dim*8, d_dim*4, 4, 2, 1), ## 32x32 image (ch: 128 to 64) | 32x32x64\n",
        "            nn.BatchNorm2d(d_dim*4),\n",
        "            nn.ReLU(True),            \n",
        "\n",
        "            nn.ConvTranspose2d(d_dim*4, d_dim*2, 4, 2, 1), ## 64x64 image (ch: 64 to 32) | 64x64x32\n",
        "            nn.BatchNorm2d(d_dim*2),\n",
        "            nn.ReLU(True),            \n",
        "\n",
        "            nn.ConvTranspose2d(d_dim*2, 3, 4, 2, 1), ## 128x128 image (ch: 32 to 3) | 128x128x3\n",
        "            nn.Tanh() ### produce result in the range from -1 to 1\n",
        "    )\n",
        "\n",
        "  #--- Function to project and reshape noise\n",
        "  def forward(self, noise):\n",
        "    x=noise.view(len(noise), self.z_dim, 1, 1)  # 128 batch x 200 no. of channels x 1 x 1 | len(noise) = batch size = 128\n",
        "    print('Noise size AFTER reshape: ', x.shape)\n",
        "    return self.gen(x) #--- we input noise in generator(gen) network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9JC08lniu4I"
      },
      "outputs": [],
      "source": [
        "#--- Function that generates noise\n",
        "def gen_noise(num, z_dim, device='cuda'):\n",
        "  noise_vector = torch.randn(num, z_dim, device=device)\n",
        "  print('Noise size: ', noise_vector, '\\n', 'Noise size BEFORE reshape: ', noise_vector.shape)\n",
        "  return  noise_vector # 128(batch size) x 200 (dimension of latent space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0r_DRWjXIn-v"
      },
      "outputs": [],
      "source": [
        "noise= gen_noise(128,200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIwmHe3shX2S"
      },
      "source": [
        "# **Critic Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNbxakfcyoB5"
      },
      "outputs": [],
      "source": [
        "## critic model\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  def __init__(self, d_dim=16):\n",
        "    super(Critic, self).__init__()\n",
        "\n",
        "    self.crit = nn.Sequential(\n",
        "      # Conv2d: in_channels, out_channels, kernel_size, stride=1, padding=0\n",
        "      ## New width and height: # (n+2*pad-ks)//stride +1\n",
        "      ## we decrease size of image and increase number of channels\n",
        "\n",
        "      #-- we start with image of 128x128x3\n",
        "      nn.Conv2d(3, d_dim, 4, 2, 1), #(n+2*pad-ks)//stride +1 = (128+2*1-4)//2+1=64x64 (ch: 3 to 16) | 64x64x16\n",
        "      nn.InstanceNorm2d(d_dim), \n",
        "      nn.LeakyReLU(0.2),\n",
        "\n",
        "      nn.Conv2d(d_dim, d_dim*2, 4, 2, 1), ## 32x32 (ch: 16 to 32) | 32x32x32\n",
        "      nn.InstanceNorm2d(d_dim*2), # Norm applied to previous layers\n",
        "      nn.LeakyReLU(0.2),\n",
        "\n",
        "      nn.Conv2d(d_dim*2, d_dim*4, 4, 2, 1), ## 16x16 (ch: 32 to 64) | 16x16x64\n",
        "      nn.InstanceNorm2d(d_dim*4), \n",
        "      nn.LeakyReLU(0.2),\n",
        "              \n",
        "      nn.Conv2d(d_dim*4, d_dim*8, 4, 2, 1), ## 8x8 (ch: 64 to 128) | 8x8x128\n",
        "      nn.InstanceNorm2d(d_dim*8), \n",
        "      nn.LeakyReLU(0.2),\n",
        "\n",
        "      nn.Conv2d(d_dim*8, d_dim*16, 4, 2, 1), ## 4x4 (ch: 128 to 256) | 4x4x256\n",
        "      nn.InstanceNorm2d(d_dim*16), \n",
        "      nn.LeakyReLU(0.2),\n",
        "\n",
        "      nn.Conv2d(d_dim*16, 1, 4, 1, 0), #(n+2*pad-ks)//stride +1=(4+2*0-4)//1+1= 1X1 (ch: 256 to 1) | 1x1x1\n",
        "      #-- we end with image of 1x1x1 - single output(real or fake)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, image):\n",
        "    # image: 128 x 3 x 128 x 128: batch x channels x width x height\n",
        "    crit_pred = self.crit(image) # 128 x 1 x 1 x 1: batch x  channel x width x height | one single value for each 128 image in batch\n",
        "    y = crit_pred.view(len(crit_pred),-1) ## 128 x 1 \n",
        "    print('Output layer shape: ',y.shape) \n",
        "    return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9tYEM_N2i87"
      },
      "source": [
        "## Initialize Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wV46jNyptu1"
      },
      "outputs": [],
      "source": [
        "## optional, init your weights in different ways\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m,nn.ConvTranspose2d):\n",
        "      torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "      torch.nn.init.constant_(m.bias,0)\n",
        "\n",
        "    if isinstance(m,nn.BatchNorm2d):\n",
        "      torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "      torch.nn.init.constant_(m.bias,0)\n",
        "\n",
        "\n",
        "##gen=gen.apply(init_weights)\n",
        "##crit=crit.apply(init_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOFwZ3ZNh8VJ"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mpx1YPgUp03-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_z0Zh8f3Uj6"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "import gdown, zipfile, tarfile\n",
        "\n",
        "url = '/content/drive/MyDrive/CelebA/archive.zip'\n",
        "path='data/celeba'\n",
        "download_path=f'{path}/img_align_celeba.zip'\n",
        "print(download_path)\n",
        "\n",
        "if not os.path.exists(path):\n",
        "  os.makedirs(path)\n",
        "\n",
        "# gdown.download(url, download_path, quiet=False)\n",
        "\n",
        "with zipfile.ZipFile(url, 'r') as ziphandler:\n",
        "  ziphandler.extractall('/content/data/celeba/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0lcXtgz6fDQ"
      },
      "outputs": [],
      "source": [
        "### Dataset, DataLoader, declare gen,crit, test dataset\n",
        "\n",
        "class Dataset(Dataset):\n",
        "  def __init__(self, path, size=128, lim=10000): #image size: 128x128 | no. of images: 10000\n",
        "    self.sizes=[size, size]\n",
        "    items, labels=[],[]\n",
        "\n",
        "    for data in os.listdir(path)[:lim]:\n",
        "      #path: './data/celeba/img_align_celeba'\n",
        "      #data: '114568.jpg\n",
        "      item = os.path.join(path,data)\n",
        "      items.append(item)\n",
        "      labels.append(data)\n",
        "    self.items=items\n",
        "    self.labels=labels\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.items)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    data = PIL.Image.open(self.items[idx]).convert('RGB') # (178,218)\n",
        "    data = np.asarray(torchvision.transforms.Resize(self.sizes)(data)) # 128 x 128 x 3 - resize image and convert to np array\n",
        "    data = np.transpose(data, (2,0,1)).astype(np.float32, copy=False) # 3 x 128 x 128 # from 0 to 255 \n",
        "    data = torch.from_numpy(data).div(255) # from 0 to 1 | \n",
        "    return data, self.labels[idx]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ9o5_PRQr9d"
      },
      "source": [
        "### Initialize Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDCXR-miPz4x"
      },
      "outputs": [],
      "source": [
        "## Dataset\n",
        "data_path='./data/celeba/img_align_celeba/img_align_celeba'\n",
        "ds = Dataset(data_path, size=128, lim=10000)\n",
        "\n",
        "## DataLoader\n",
        "dataloader = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "## Models\n",
        "gen = Generator(z_dim).to(device)\n",
        "crit = Critic().to(device)\n",
        "\n",
        "## Optimizers\n",
        "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(0.5,0.9))\n",
        "crit_opt= torch.optim.Adam(crit.parameters(), lr=lr, betas=(0.5,0.9))\n",
        "\n",
        "## Initializations\n",
        "##gen=gen.apply(init_weights)\n",
        "##crit=crit.apply(init_weights)\n",
        "\n",
        "#wandb optional\n",
        "if (wandbact==1):\n",
        "  wandb.watch(gen, log_freq=100)\n",
        "  wandb.watch(crit, log_freq=100)\n",
        "\n",
        "x,y=next(iter(dataloader))\n",
        "show(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXrWl2z52IX0"
      },
      "source": [
        "## Gradient Penalty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaTz8qmn9h6-"
      },
      "outputs": [],
      "source": [
        "## gradient penalty calculation\n",
        "\n",
        "def get_gp(real, fake, crit, epsilon, xlambda=10):\n",
        "  interpolated_images = real * epsilon + fake * (1-epsilon) # 128 x 3 x 128 x 128 | Linear Interpolation\n",
        "  interpolated_scores = crit(interpolated_images) # 128 x 1 | prediction of critic\n",
        "\n",
        "  # Analyze gradients if too large\n",
        "  gradient = torch.autograd.grad(\n",
        "      inputs = interpolated_images,\n",
        "      outputs = interpolated_scores,\n",
        "      grad_outputs=torch.ones_like(interpolated_scores),\n",
        "      retain_graph=True,\n",
        "      create_graph=True,\n",
        "  )[0] # 128 x 3 x 128 x 128\n",
        "\n",
        "  gradient = gradient.view(len(gradient), -1)   # 128 x 49152\n",
        "  gradient_norm = gradient.norm(2, dim=1)  # L2 norm\n",
        "  gp = xlambda * ((gradient_norm-1)**2).mean()\n",
        "\n",
        "  return gp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5mVIgE72Lx3"
      },
      "source": [
        "## Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JW9H2tlICYI8"
      },
      "outputs": [],
      "source": [
        "## Save and load checkpoints\n",
        "\n",
        "root_path='./data/'\n",
        "\n",
        "def save_checkpoint(name):\n",
        "  torch.save({\n",
        "      'epoch': epoch,\n",
        "      'model_state_dict': gen.state_dict(),\n",
        "      'optimizer_state_dict': gen_opt.state_dict()      \n",
        "  }, f\"{root_path}G-{name}.pkl\")\n",
        "\n",
        "  torch.save({\n",
        "      'epoch': epoch,\n",
        "      'model_state_dict': crit.state_dict(),\n",
        "      'optimizer_state_dict': crit_opt.state_dict()      \n",
        "  }, f\"{root_path}C-{name}.pkl\")\n",
        "  \n",
        "  print(\"Saved checkpoint\")\n",
        "\n",
        "def load_checkpoint(name):\n",
        "  checkpoint = torch.load(f\"{root_path}G-{name}.pkl\")\n",
        "  gen.load_state_dict(checkpoint['model_state_dict'])\n",
        "  gen_opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "  checkpoint = torch.load(f\"{root_path}C-{name}.pkl\")\n",
        "  crit.load_state_dict(checkpoint['model_state_dict'])\n",
        "  crit_opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "  print(\"Loaded checkpoint\")\n",
        "\n",
        "#load_checkpoint('final-wgan-noinit')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8QlaEw4EXED"
      },
      "outputs": [],
      "source": [
        "#!cp C-final* ./data/\n",
        "#!cp G-final* ./data/\n",
        "#epoch=1\n",
        "#save_checkpoint(\"test\")\n",
        "#load_checkpoint(\"test\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXlwjLuc2OAC"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Kt-QSmiEhG6"
      },
      "outputs": [],
      "source": [
        "## Training loop \n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  for real, _ in tqdm(dataloader):\n",
        "    cur_bs= len(real) #128\n",
        "    real=real.to(device)\n",
        "\n",
        "    '''Critic Training'''\n",
        "\n",
        "    mean_crit_loss = 0\n",
        "    for _ in range(crit_cycles):\n",
        "      #--- Initialize Gradients to 0\n",
        "      crit_opt.zero_grad()\n",
        "      \n",
        "      #--- Create Noise\n",
        "      noise=gen_noise(cur_bs, z_dim)\n",
        "      #---Create Fake Image from Noise\n",
        "      fake = gen(noise)\n",
        "\n",
        "      #--- Get prediction on fake and real image\n",
        "      crit_fake_pred = crit(fake.detach())\n",
        "      crit_real_pred = crit(real)\n",
        "\n",
        "      #--- Calculate gradient penalty\n",
        "      epsilon=torch.rand(len(real),1,1,1,device=device, requires_grad=True) # 128 x 1 x 1 x 1\n",
        "      gp = get_gp(real, fake.detach(), crit, epsilon)\n",
        "\n",
        "      #--- Calculate Critic Loss\n",
        "      crit_loss = crit_fake_pred.mean() - crit_real_pred.mean() + gp\n",
        "      mean_crit_loss+=crit_loss.item() / crit_cycles\n",
        "\n",
        "      #--- Backpropagation\n",
        "      crit_loss.backward(retain_graph=True)\n",
        "      #--- Update parameter of critic\n",
        "      crit_opt.step()\n",
        "\n",
        "    #--- Append Critic Loss\n",
        "    crit_losses+=[mean_crit_loss]\n",
        "\n",
        "    #---------------------------------------------------------------------------------------------------------------------------------#\n",
        "\n",
        "    '''Generator Training'''\n",
        "    #--- Initialize Gradients to 0\n",
        "    gen_opt.zero_grad()\n",
        "\n",
        "    #---Create Noise Vector\n",
        "    noise = gen_noise(cur_bs, z_dim)\n",
        "    #---Create Fake image from Noise vector\n",
        "    fake = gen(noise)\n",
        "\n",
        "    #---Critic's prediction on fake image\n",
        "    crit_fake_pred = crit(fake)\n",
        "\n",
        "    #--- Calculate Generator Loss\n",
        "    gen_loss = -crit_fake_pred.mean()\n",
        "\n",
        "    #---Backpropagation\n",
        "    gen_loss.backward()\n",
        "    #--- Update generator's paramaters\n",
        "    gen_opt.step()\n",
        "\n",
        "    #--- Append Generator Loss\n",
        "    gen_losses+=[gen_loss.item()]\n",
        "\n",
        "    #---------------------------------------------------------------------------------------------------------------------------------#\n",
        "\n",
        "    ### Stats  \n",
        "   \n",
        "    if (wandbact==1):\n",
        "      wandb.log({'Epoch': epoch, 'Step': cur_step, 'Critic loss':mean_crit_loss, 'Gen loss': gen_loss})\n",
        "\n",
        "    if cur_step % save_step == 0 and cur_step > 0:\n",
        "      print(\"Saving checkpoint: \", cur_step, save_step)\n",
        "      save_checkpoint(\"latest\")\n",
        "\n",
        "    if (cur_step % show_step == 0 and cur_step > 0):\n",
        "      show(fake, wandbactive=1, name='fake')\n",
        "      show(real, wandbactive=1, name='real')\n",
        "\n",
        "      gen_mean=sum(gen_losses[-show_step:]) / show_step\n",
        "      crit_mean = sum(crit_losses[-show_step:]) / show_step\n",
        "      print(f\"Epoch: {epoch}: Step {cur_step}: Generator loss: {gen_mean}, critic loss: {crit_mean}\")\n",
        "\n",
        "      plt.plot(\n",
        "          range(len(gen_losses)),\n",
        "          torch.Tensor(gen_losses),\n",
        "          label=\"Generator Loss\"\n",
        "      )\n",
        "\n",
        "      plt.plot(\n",
        "          range(len(gen_losses)),\n",
        "          torch.Tensor(crit_losses),\n",
        "          label=\"Critic Loss\"\n",
        "      )\n",
        "\n",
        "      plt.ylim(-150,150)\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "    \n",
        "    cur_step+=1\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKAsUymMvcKX"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkgGRFqMHSII"
      },
      "outputs": [],
      "source": [
        "# number of steps per epoch\n",
        "# 10000 / 128 = 78.125\n",
        "# 50000 / 128 = 390.625"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncffzc712Qz9"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53_FHIdBps4K"
      },
      "outputs": [],
      "source": [
        "#### Generate new faces\n",
        "noise = gen_noise(batch_size, z_dim)\n",
        "fake = gen(noise)\n",
        "show(fake)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QfYviN1nrSD"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9beiHfjffOD"
      },
      "source": [
        "### Get one Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2Bo_j9ewSAu"
      },
      "outputs": [],
      "source": [
        "plt.imshow(fake[24].detach().cpu().permute(1,2,0).squeeze().clip(0,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYg9vD4_flbg"
      },
      "source": [
        "## Morphing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOgMT09WxQFW"
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "\n",
        "# MORPHING, interpolation between points in latent space\n",
        "gen_set=[]\n",
        "z_shape=[1,200,1,1]\n",
        "rows=4\n",
        "steps=17\n",
        "\n",
        "for i in range(rows):\n",
        "  z1,z2 = torch.randn(z_shape), torch.randn(z_shape)\n",
        "  for alpha in np.linspace(0,1,steps):\n",
        "    z=alpha*z1 + (1-alpha)*z2\n",
        "    res=gen(z.cuda())[0]\n",
        "    gen_set.append(res)\n",
        "\n",
        "fig = plt.figure(figsize=(25,11))\n",
        "grid=ImageGrid(fig, 111, nrows_ncols=(rows,steps), axes_pad=0.1)\n",
        "\n",
        "for ax , img in zip(grid, gen_set):\n",
        "  ax.axis('off')\n",
        "  res=img.cpu().detach().permute(1,2,0)\n",
        "  res=res-res.min()\n",
        "  res=res/(res.max()-res.min())\n",
        "  ax.imshow(res.clip(0,1.0))\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "DCGAN: Generating Faces using CelebA Dataset",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}